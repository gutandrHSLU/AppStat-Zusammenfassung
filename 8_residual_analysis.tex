\section{Residual Analysis}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDickeDick}


\subsection{Introduction}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
In our simple linear regression models we made the following assumptions:
\begin{enumerate}
  \item The relationship between response and the regressors is linear.
  \item The error $\varepsilon$ has mean zero.
  \item The error $\varepsilon$ has constant variance $\sigma^2$
  \item The errors are uncorrelated.
  \item The errors are normally distributed
\end{enumerate}
If some assumptions are violated we should be able to see them in the errors $\varepsilon_i$. On the other hand, the errors are unknown to us, so we have to deal with the residuals
\begin{equation}
  e_i = y_i - \hat{y}_i \;\;\;\text{for all}\;\;\; i \in {1,...,n}
\end{equation}
instead. The residuals are estimators of the random errors.\\

\textbf{Scaled Residuals}\\
If the errors are normally distributed, so are the residuals of a least-squares estimate. Since the variance of the residuals depends on $x_0$ it is not equal to the variance of the errors. Therefore we use scaled residuals
\begin{equation}
  \tilde{e}_i = \frac{e_i}{\sqrt{1 - \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{S_{xx}}\right)}} \;\;\;\text{for all}\;\;\; i \in {1,...,n}
\end{equation}


\textbf{Coefficient of Determination}\\
The coefficient of determination is a measure of the linear relationship between the response variable and the fit.\\

In simple linear regression with only one explanatory variable the coefficient of determination is defined by
\begin{equation}
  R^2 = \frac{SS_{\textup{fit}}}{S_{yy}}
\end{equation}
where
\begin{equation}
  \begin{split}
    SS_{\textup{fit}} &= \sum_{i=1}^n(\hat{y}_i - \bar{\hat{y}}_i)^2 = \hat{\beta}_1^2 S_{xx}\\
    SS_{yy} &= \sum_{i=1}^n(y_i - \bar{y}_i)^2
  \end{split}
\end{equation}
Therefore the coefficient of determination is
\begin{equation}
  R^2 = \hat{\beta}_1^2 \frac{S_{xx}}{S_{yy}} = \frac{S_{xy}^2}{S_{xx} S_{yy}} = \textup{Cor}(x, y)^2
\end{equation}

In multiple regression and simple linear regression with at least one explanatory variable the coefficient of determination is identical to the squared correlation between the response variable $y$ and the fitted values $\hat{y}$
\begin{equation}
  R^2 = \textup{Cor}(y, \hat{y})^2 = \frac{S_{y\hat{y}}^2}{S_{yy} S_{\hat{y}\hat{y}}}
\end{equation}

The coefficient of determination is a global measure for the goodness of fit and it says nothing about the suitability of the regression model.

\subsection{Diagnostic Tools}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
What follows is a description of three graphical diagnostic tools to check model assumptions 1, 2, 3 and 5. The aim is to be sure that there are no dangerous discrepancies from the assumptions in the data. All three tools are based on the residuals which are representations of the unknown errors.\\

\textbf{Tukey-Anscombe Plot}\\
The idea of this plot is to plot residuals $e_i$ versus fitted values $\hat{y}_i$.
With the Tukey-Anscombe Plot we can check whether the second model assumption holds true.\\
Residuals in any interval of the Tukey-Anscombe plot should vary randomly around the horizontal line at zero.

\begin{table}[H]
  \setlength{\tabcolsep}{0.2em}
  \scriptsize
  \begin{tabular}{p{\linewidth / 2 - 0.5em}@{\hskip 1em}p{\linewidth / 2 - 0.5em}}
    \includegraphics[width=\linewidth]{Pics/8.2.1.png}& \includegraphics[width=\linewidth]{Pics/8.2.2.png} \\
    Best linear model &
    Tukey-Anscombe plot of the model. The points are not spread uniformly around the horizontal line indicating a bad model.\\
  \end{tabular}
\end{table}

Model assumption number two states that the errors should have mean zero. To check this assumption it is best to smooth the data in the
Tukey-Anscombe plot. We are still left with the question: Is such a curved curve due to chance possible? An informal method to find answers is based on bootstrap simulations\\

\textbf{Bootstrap Simulation}\\
What follows is a briev explanation of the bootstrap simulation:\\
- Calculate the best linear model with $\textup{Var} = \hat{\sigma}^2$ and calculate the smooth curve of the residuals.\\
- Simulate new observations based off the fitted model.
- Calculate the smooth residuals of the simulated obervation and add it to the diagram.\\
- Repeat 19 times. (1 + 19 observations)
\begin{table}[H]
  \setlength{\tabcolsep}{0.2em}
  \scriptsize
  \begin{tabular}{p{\linewidth / 2 - 0.5em}@{\hskip 1em}p{\linewidth / 2 - 0.5em}}
    \includegraphics[width=\linewidth]{Pics/8.2.5.png}& \includegraphics[width=\linewidth]{Pics/8.2.6.png} \\
    Tukey-Anscombe plot with smoothed residuals (red). The smoothed curve shows a clear curvature (which is bad). &
    Tukey-Anscombe plot with 19 simulated smoothed residuals (gray). The curved smooth curve is not an extreme curve among all 20. We conclude that the model fits the data.\\
  \end{tabular}
\end{table}

\textbf{Scale-Location Plot}\\
The idea of this plot is to plot the residuals of square-root of absolute standardised residuals $\sqrt{|\tilde{e}_{std, i}|}$ versus fitted values $\hat{y}_i$. With the scale-location plot we can check whether the third model assumption holds true.\\
If the smoothed curve of square-root of the absolute standardised residuals is approximately horizontal, then the errors have equal variance.
\begin{table}[H]
  \setlength{\tabcolsep}{0.2em}
  \scriptsize
  \begin{tabular}{p{\linewidth / 2 - 0.5em}@{\hskip 1em}p{\linewidth / 2 - 0.5em}}
    \includegraphics[width=\linewidth]{Pics/8.2.7.png}& \includegraphics[width=\linewidth]{Pics/8.2.8.png} \\
    Scale-location plot with smoothed residuals (red). We observe a clear downward trend in residual scattering. &
    Scale-location plot with 19 simulated smoothed residuals (gray). The simulation confirms the extraordinary behavior.\\
  \end{tabular}
\end{table}

\textbf{Normal q-q Plot (and Histogram)}\\
The idea of the histogram is to compare histogram of residuals $e_i$ with normal density function with parameters $0$ and $\hat{\sigma}^2$. It is often difficult to compare a histogram with a bell shaped curve and the histogram is very sensitive to the number of histogram cells and the breakpoints between cells.\\
The idea of the normal q-q plot is to plot quantiles of the empirical distribution of the of the standardised residuals $\tilde{e}_{std, i}$ versus quantiles of the normal distribution.\\
If the data is from a normal distribution, the points in the q-q plot scatter around a straight line.

\begin{table}[H]
  \setlength{\tabcolsep}{0.2em}
  \scriptsize
  \begin{tabular}{p{\linewidth / 2 - 0.5em}@{\hskip 1em}p{\linewidth / 2 - 0.5em}}
    \includegraphics[width=\linewidth]{Pics/8.2.10.png}& \includegraphics[width=\linewidth]{Pics/8.2.11.png} \\
    q-q plot with standardised residuals. We can observe a discrepancy to normality &
    q-q plot with 19 simulated standardised residuals (gray). The simulation shows us that this discrepancy might be due to random fluctuations\\
  \end{tabular}
\end{table}

Todo: Understand left and right-skewed plots

\subsection{Treatment of Model Violations}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}

\textbf{Non-Constant Variance of Random Errors}\\
Often the assumption is violated that the variance of random errors is constant.\\
This model violation can be changed by a transformation of the response variable. If the standard deviation of the residuals is more or less proportional to the response variable, then a logarithmic transformation of the response variable might help. If this is a too strong transformation then a square-root transformation might be more adequate.
\begin{itemize}
  \item \textbf{Logarithmic} for continuous positive variables:
  \begin{equation}
    z \mapsto \textup{log}(z)
  \end{equation}
  \item \textbf{Square-root} for continuous and discrete positive variables:
  \begin{equation}
    z \mapsto \sqrt{z}
  \end{equation}
  \item \textbf{Arcsine} for proportions:
  \begin{equation}
    z \mapsto \textup{arcsin}(\sqrt{z})
  \end{equation}
  \item \textbf{Logit} for proportions:
  \begin{equation}
    z \mapsto \textup{log}\left(\frac{z + \varepsilon_1}{1 + \varepsilon_2 - z} \right)
  \end{equation}
\end{itemize}

\textbf{Outliers}\\
The term outlier is not clearly defined. It is an observation that fits badly with a model that is suitable for most data. In the case of an univariate sample, an outlier is an observation that is, compared to the scattering of the data, far away from the median.\\

\textbf{Heavy-Tailed Distributions}\\
If the q-q plot shows a symmetric distribution but with heavy tails, then transformations are useless. One possibility is to omit the most extreme observations such that the heavy tails disappear. In statistics this is often called trimming. Results obtained with a trimmed data set should be treated with care: the error probability of statistical tests will be wrong.

\subsection{Independence}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
If the observations are ordered chronologically, then it can happen that errors are correlated, i.e. neighbor residuals $e_i$ are more similar than residuals far apart. In such a situation we say that the errors are autocorrelated and the model assumption are violated. If the errors are correlated then we can see a certain pattern if we plot the residuals versus the subsequent residuals in a scatter plot.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Pics/8.4.1.png}
\end{figure}

\begin{itemize}
  \item \textbf{Negative correlation}\\
  After a positive (negative) residual the chance of observing a negative (positive) residual is high, i.e. very strong alternation of the sign of the residuals.\\
  In the scatter plot the points lie in an ellipse with negative slope of the first principal axis.
  \item \textbf{No correlation}\\
  The residuals are completely random, no obvious pattern present.\\
  In the scatter plot the points show no pattern.
  \item \textbf{Positive correlation}
  After a positive (negative) residual the chance of observing a positive (negative) residual is high, i.e. the residuals show the same sign over certain periods.\\
  In the scatter plot the points lie in an ellipse with positive slope of the first principal axis.
\end{itemize}
