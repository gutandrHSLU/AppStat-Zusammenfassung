\section{Simple Linear Regression}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDickeDick}
One of the most important and widely used statistical technique is regression analysis. This is a statistical technique for investigating and modelling relationships between variables.

\subsection{Simple Linear Regression Model}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
A simple linear regression model is a model with a single predictor variable that has a relationship with a response that is a straight line.\\

\textbf{Model}
\begin{equation}
  y = \beta_0 + \beta_1x \cdot \varepsilon
\end{equation}
where $\beta_0$ and $\beta_1$ are unknown and fixed parameters.\\

\textbf{Variables and error}\\
- Perdictor / input / explanatory variable x (deterministic)\\
- Response / output variable y (random variable)\\
- Uncorrelated random error $\varepsilon $ with
\begin{equation}
  E(\varepsilon ) = 0
\end{equation}
and unknown variance
\begin{equation}
  \textup{Var}(\varepsilon ) = \sigma^2
\end{equation}
Since the error is a random value, y is also a random value.\\

\textbf{Aim}\\
Find Parameters $\hat{\beta}_0, \hat{\beta}_1$ and $\hat{\sigma}^2$ such that model fits data as well as possible.

\textbf{Assumptions}\\
The error $\varepsilon $ is normaly distributed:
\begin{equation}
  E \approx \mathcal{N}(0, \sigma^2) \Rightarrow E(y) = \beta_0 + \beta_1 x
\end{equation}

\subsection{Estimation of the Parameters}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
\textbf{Method of least square}\\
Minimise:
\begin{equation}
  S(\beta_0, \beta_1) = \sum_{i=1}^n(y_i-(\beta_0 + \beta_1x_i))^2
\end{equation}

Calculating:
\begin{equation}
  \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x} \;\;\text{with}\;\; \hat{\beta}_1 = \frac{S_{xy}}{S_{xx}}
\end{equation}
where
\begin{equation}
  S_{xx} =  \sum^n_{i=1}(x_i - \bar{x})^2 \;\;\text{and}\;\; S_{xy} =  \sum^n_{i=1}(x_i - \bar{x})(y_i - \bar{y})
\end{equation}
with
\begin{equation}
  \bar{x} = \frac{1}{n}\sum_{i=1}^n x_i  \;\;\text{and}\;\; \bar{y} = \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}

\textbf{Residual}\\
The difference between the $i$th observed value $y_i$ and its fitted value $\hat{y_i}$ is the residual
\begin{equation}
  e_i = y_i - \hat{y}_i = y_i - (\beta_0 + \beta_1x_i)
\end{equation}

\textbf{Mean of residuals}\\
\begin{equation}
  \bar{e} = \frac{1}{n} \sum_{i=1}^n e_i = 0
\end{equation}

\textbf{Unbiased estimator}\\
An unbiased estimator $\hat{\sigma}^2$  is obtained from the error sum of squares
\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n - 2} \sum_{i=1}^n (e_i - \bar{e})^2 = \frac{1}{n - 2} \sum_{i=1}^n e_i^2
\end{equation}
Notice that we have to divide by the degrees of freedom $n - 2$ to make the estimator unbiased. (2 because we have two parameters to find).\\

\textbf{Distribution of the estimators}\\
To be able to answer how well the model fits the data, whether the model can be used as a reliable predictor and whether the the assumptions of constant variance and uncorrelated errors are met, we need to know the distributions of the estimators $\beta_0$ and $\beta_1$.\\
Since the estimators $\beta_0$ and $\beta_1$ are calculated using the random variable $y$, the estimators themselves are random variables which follow the distributions
\begin{equation}
  \hat{\beta}_0 \sim \mathcal{N}\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)
\end{equation}
and
\begin{equation}
  \hat{\beta}_1 \sim \mathcal{N}\left(\beta_1, \frac{\sigma^2}{S_{xx}}\right).
\end{equation}

\subsection{Tests and Confidence Intervals}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
In many cases we are not only interested in estimating the model parameters but also in testing hypothesis and constructing confidence intervals.\\

\paragraph{Statistical Test on the Slope}\mbox{}\\
\textbf{Hypothesis}
\begin{equation}
  \begin{split}
    H_0:& \beta_1 = \beta_{1,0}\\
    H_1:& \beta_1 \neq \beta_{1,0}
  \end{split}
\end{equation}
The null hypothesis states that the observations follow the model of simple linear regression with $\beta_{1} = \beta_{1.0}$ and arbitrary $\beta_0$ and $\sigma$. Where $\beta_{1.0}$ is the slope we want to test.\\

\textbf{Estimated standard error}\\
We can estimate the standard error with the error sum of squares.
\begin{equation}
  se(\hat{\beta}_1) = \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}} \text{    with    } S_{xx} =  \sum^n_{i=1}(x_i - \bar{x})^2
\end{equation}

\textbf{Test statistic}
\begin{equation}
  T = \frac{\hat{\beta}_1 - \beta_{1,0}}{se(\hat{\beta}_1)}
\end{equation}
Under the null hypothesis the test statistic $T$ follows a Studentâ€™s t-distribution with $n - 2$ degrees of freedom. Values of t are usually found in a table.\\
If T is smaller than t (from the table), we accept the null hypothesis and conclude that the data agrees also with a model with the slope $\beta_{1,0}$.


\textbf{P-Value}\\
The probability, under the null hypothesis, of obtaining a result equal to or more extreme than what was actually estimated. If the value T of the test statistic is larger than the critival value t (from the table) the null hypothesis is not rejected i.e. it is plausible to assume that the data fits the model.

\paragraph{Confidence Test}\mbox{}\\
The test statistic
\begin{equation}
  T = \frac{\hat{\beta}_1 - \beta_{1,0}}{se(\beta_1)}
\end{equation}
is accepted on the significance level $\alpha$ if
\begin{equation}
  t_{\frac{\alpha}{2}, n-2} \leq T \leq t_{-\frac{\alpha}{2}, n-2}
\end{equation}



\textbf{Confidence interval on the slope}
\begin{equation}
  \hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2} \cdot se(\beta_1) \leq \beta_1 \leq  \hat{\beta}_1 + t_{-\frac{\alpha}{2}, n-2} \cdot se(\beta_1)
\end{equation}
\begin{equation}
  \left[\hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2} \cdot se(\beta_1), \hat{\beta}_1 - t_{-\frac{\alpha}{2}, n-2} \cdot se(\beta_1)\right]
\end{equation}

Interpretation:
The true value lies in the confidence interval with high probability.

\subsection{Confidence Interval on Response and Prediction Intervals}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
\textbf{Confidence Interval of the Response}\\
Estimated response
\begin{equation}
  \hat{y}_0 = \hat{\beta}_0 +  \hat{\beta}_1 x_0
\end{equation}
\begin{equation}
  \textup{Var}(\hat{y}_0) = \sigma^2\left(\frac{1}{n}) + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right)
\end{equation}

\begin{equation}
  T = \frac{\hat{y}_0 - \mu_0}{se(\hat{y}_0)}
\end{equation}

\begin{equation}
  se(\hat{y}_0)=\hat{\sigma}\sqrt{\frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}
\end{equation}

se = standard error
\textbf{confidence interval on the response}
\begin{equation}
  \hat{y}_0 - t \leq \hat{\beta}_0 +  \hat{\beta}_1 x_0 \leq \hat{y}_0 + t
\end{equation}
where
\begin{equation}
  t = \frac{\alpha}{2},n-2 \cdot se(\hat{y}_0)
\end{equation}

\textbf{Prediction Interval}\\

\begin{equation}
  \begin{align}
      \textup{Var}(y_0 - \hat{y}_0) &= \textup{Var}(y_0) + \textup{Var}(\hat{y}_0)\\
      &= \sigma^2\left(\frac{1}{n}) + \frac{(x_0 - \bar{x})^2}{S_{xx}}\right)
  \end{align}
\end{equation}
