\section{Multiple Linear Regression}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDickeDick}
A multiple regression model is a model which involves more than one regressor variable.\\

\subsection{Model and Estimation}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
We generalise the concept of a simple linear regression model with only one explanatory variable to a multiple linear regression model
\begin{equation}
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_m x_m +\varepsilon,
\end{equation}

$\beta_0$ is the intercept\\
$\beta_k$ is the slope in the $x_k$ direction for all $k \in {1, 2, ..., m}$.\\

\textbf{Least-Squares Estimation of the Regression Coefficient}\\
The general idea and philosophy of the multiple linear regression model are the same as in the simple linear regression model. We will again estimate the coefficients $\beta_0$ and $\beta_1, \beta_2, . . . , \beta_m$ with the method of least-squares. To do this we will use matrix notation.
\begin{equation}
  \mathbf{y} = \mathbf{X} \pmb{\beta} + \varepsilon
\end{equation}
Now we want to find the vector of least-squares estimators $\pmb{\beta}$, which minimise
\begin{equation}
  S(\pmb{\beta}) = \sum^n_{i=1} \varepsilon^2_i = \pmb{\varepsilon}^t\pmb{\varepsilon} = (\mathbf{y} - \mathbf{X}\pmb{\beta})^t (\mathbf{y} - \mathbf{X}\pmb{\beta})
\end{equation}
The solution of the normal equations is the least-squares estimator
\begin{equation}
  \hat{\pmb{\beta}} = (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t \mathbf{y}
\end{equation}

It is now straight forward to calculate the fitted values
\begin{equation}
  \hat{\mathbf{y}} = \mathbf{X}\hat{\pmb{\beta}} = \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t \mathbf{y} = \mathbf{H}\mathbf{y}
\end{equation}

The matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t$ is usually called the \textbf{hat matrix}. It maps the vector of observed values $\mathbf{y}$ into a vector of fitted values $\hat{\mathbf{y}}$. It puts $\mathbf{y}$ the hat on!\\
The $n \times 1$ vector of residuals e can be written as
\begin{equation}
  \mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{H}\mathbf{y} = (\mathbf{I} - \mathbf{H}) \mathbf{y}
\end{equation}
where $\mathbf{I}$ is the $n \times n$ identity matrix.\\
Aufg. 8.5.1 is important!!\\

\textbf{Tests on the Significance of any Individual Regression Coefficient}\\
It is possible to show that $\hat{\pmb{\beta}}$ follows an $(m+1)$-dimensional multivariate normal distribution with expected value
\begin{equation}
  \textup{E}(\hat{\pmb{\beta}}) = \pmb{\beta}
\end{equation}
and $(m + 1) \times (m + 1)$ covariance matrix
\begin{equation}
  \textup{Cov}(\hat{\pmb{\beta}}) = \sigma^2 (\mathbf{X}^t \mathbf{X})^{-1}
\end{equation}
Using the above we can derive a test procedure and corresponding confidence intervals for
each coefficient $\beta_0$ and $\beta_k$ for $k \in \{1, 2, . . . , m\}$. The hypothesis for testing the significance of any individual regression coefficient, such as $\beta_k$, are
\begin{equation}
  \begin{split}
    H_0 &: \beta_k = \beta_{k,0}\\
    H_1 &: \beta_k \neq \beta_{k,0}.
  \end{split}
\end{equation}
The test statistic for this hypothesis is
\begin{equation}
  T_k = \frac{\hat{\beta}_k - \beta_{k, 0}}{\textup{se}(\hat{\beta}_k)}
  \;\;\;\; \text{with} \;\;\;\;
  \textup{se}(\hat{\beta}_k) = \hat{\sigma}\sqrt{\left(\left(\mathbf{X}^t\mathbf{X}\right)^{-1}\right)_{kk}}
\end{equation}
where $((\mathbf{X}^t\mathbf{X})^{-1})_{kk}$ is the kth diagonal element of the matrix $(\mathbf{X}^t\mathbf{X})^{-1})$. Under the null hypothesis the test statistic $T_k$ follows a Student’s $t$-distribution with $n - (m + 1)$ degrees of freedom. The degrees of freedom equals the denominator of
\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n-(m+1)}\sum^n_{i=1} e^2_i =\frac{\mathbf{e}^t \mathbf{e}}{n-(m+1)}
\end{equation}
where $\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}}$ is the $n \times 1$ vector of residuals. If $H_0 : \beta_k = 0$ is not rejected, then this indicates that the regressor $x_k$ can be omitted from the model.\\

\textbf{Confidence Intervals on the Regression Coefficients}\\
The corresponding $100(1 - \alpha)$ percent confidence interval on $\beta_k$ is
\begin{equation}
  \hat{\beta}_k - t_{\frac{\alpha}{2},n-(m+1)} \cdot \textup{se}(\hat{\beta}_k)
  \leq \beta_k \leq
  \hat{\beta}_k + t_{1-{\frac{\alpha}{2},n-(m+1)}} \cdot \textup{se}(\hat{\beta}_k),
\end{equation}
where $\textup{se}(\hat{\beta}_k)$ can be found in above and $t_{p,n-(m+1)}$ denotes the critical value of Student’s $t$-distribution to the probability $p$ and with $n - (m + 1)$ degrees of freedom\\

\textbf{Confidence Interval of the Response}\\
We want to construct a confidence interval on the response at a particular point. Define the $(m+1) \times 1$ vector
\begin{equation}
  \mathbf{x}_0 = \begin{pmatrix} 1 \\ x_{01} \\ x_{02}\\ \vdots \\ x_{0m}\end{pmatrix}
\end{equation}
and calculate the estimated value
\begin{equation}
  \hat{y}_0 = \mathbf{x}^t_0 \hat{\pmb{\beta}}
\end{equation}
This is an unbiased, normally distributed estimator with variance
\begin{equation}
  \textup{Var}(\hat{y}_0) = \sigma^2  \mathbf{x}^t_0 (\mathbf{X}^t \mathbf{X})^{-1}  \mathbf{x}_0
\end{equation}
Therefore a $100(1 - \alpha)$ percent confidence interval on the response at the point $(x_{01}, x_{02}, . . . , x_{0m})$ is
\begin{equation}
  \hat{y}_0 - t_{\frac{\alpha}{2},n-(m+1)} \cdot \textup{se}(\hat{y}_0)
  \leq \mathbf{x}^t_0 \hat{\pmb{\beta}} \leq
  \hat{y}_0 + t_{1-{\frac{\alpha}{2},n-(m+1)}} \cdot \textup{se}(\hat{y}_0),
\end{equation}
with
\begin{equation}
 \textup{se}(\hat{y}_0) = \hat{\sigma}\sqrt{ \mathbf{x}^t_0 \left(\mathbf{X}^t_0\mathbf{X}\right)^{-1} \mathbf{x}_0}
\end{equation}
and where  $t_{p,n-(m+1)}$ denotes the critical value of Student’s $t$-distribution to the probability $p$ and with $n - (m + 1)$ degrees of freedom.\\

\textbf{Prediction Interval}\\
The regression model can be used to predict future observations on $y$ corresponding to particular values of regressor variables, for example, $(x_{01}, x_{02}, . . . , x_{0m})$. A point estimate of the future observation is
\begin{equation}
  \hat{y}_0 = \mathbf{x}^t_0 \hat{\pmb{\beta}}
\end{equation}
where $\mathbf{x}_0$ was defined above.\\
A $100(1 - \alpha)$ percent prediction interval for the future observation $(x_{01}, x_{02}, . . . , x_{0m})$ is
\begin{equation} \scriptstyle
  \hat{y}_0 - t_{\frac{\alpha}{2},n-(m+1)} \cdot \sqrt{\sigma^2 + \textup{se}(\hat{y}_0)^2}
  \;\; \leq \;\; \mathbf{x}^t_0 \hat{\pmb{\beta}} \;\; \leq \;\;
  \hat{y}_0 + t_{1-{\frac{\alpha}{2},n-(m+1)}} \cdot \sqrt{\sigma^2 + \textup{se}(\hat{y}_0)^2},
\end{equation}
where $\textup{se}(\hat{y}_0)$ was defined above and $t_{p,n-(m+1)}$ denotes the critical value of Student’s $t$-distribution to the probability $p$ and with $n - (m + 1)$ degrees of freedom.\\

\textbf{Coefficient of Determination - Multiple R-Squared}\\
In multiple regression the coefficient of determination, often called multiple $R^2$, is identical to the squared correlation between the response variable $y$ and the fitted values $\hat{y}$
\begin{equation}
  R^2 = \textup{Cor}(y, \hat{y})^2.
\end{equation}
The coefficient of determination is a measure of the linear relationship between the response variable and the fit.

\subsection{Diversity of Modelling Possibilities}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
In the model of multiple regression, no assumptions are made about the explanatory variables. In all our examples, they were always continuous variables, but that is not necessary.Therefore, they do not have to be of a particular data type, let alone follow a particular
distribution, because they are not considered as random variables. Nothing is assumed about
their dependence on each other.\\
Explanatory variables can also be strongly (linearly) correlated. It is however difficult to interpret the results of the regression analysis and should be avoided.\\

\textbf{Polynomial Regression}\\
The polynomial function is an important function for formulating curved relationships. If we want to use a polynomial of degree two to describe a relationship, then the model has the following form
\begin{equation}
  y = \beta_0 + \beta_1x + \beta_2 x^2 + \varepsilon
\end{equation}
If we define the new variables $\texttt{lin} = x$ and $\texttt{quad} = x^2$, then we obtain mathematically the same model
\begin{equation}
  y = \beta_0 + \beta_1 \cdot \texttt{lin} + \beta_2 \cdot \texttt{quad} + \varepsilon
\end{equation}
which is now a multiple linear regression model.\\

\textbf{Nonlinear Functions and Linear Regression}\\
Experience shows that in most real-world regression problems, the variables must be transformed to get the best results. Four examples:
\begin{itemize}
  \item Inverting the nonlinear equations
  \begin{align*}
    y =& \frac{1}{a+be^{-x}}  &  \;&\text{gives} & \frac{1}{y} =& a + be^{-x}\\
    y =& \frac{ax}{b+x}       &  \;&\text{gives} & \frac{1}{y} =& \frac{1}{a} + \frac{a}{b} \frac{1}{x}
  \end{align*}
  \item Taking logarithms of the nonlinear equations
  \begin{align*}
    y &= ax^b       & \;&\text{gives} & \textup{ln}(y) &= \textup{ln}(a) + b\textup{ln}(x)\\
    y &= ax^{bg(x)} & \;&\text{gives} & \textup{ln}(y) &= \textup{ln}(a) + bg(x)
  \end{align*}
\end{itemize}
It is important to realise that also such transformations change the form of the error term. If we are not allowed to alter the original additive error term, then we need to use nonlinear least-squares regression.\\

\textbf{Binary Explanatory Variables}\\
In all our examples, the explanatory variables have been continuous so far. This is not necessary.\\
If the binary variable $x_b$ is the only variable in the model $y = \beta_0 + \beta_1 x_b + \varepsilon$ then
\begin{align*}
  y =& \beta_0 + \varepsilon           & \textup{if}& & x_b =& 0\\
  y =& \beta_0 + \beta_1 + \varepsilon & \textup{if}& & x_b =& 1
\end{align*}

This regression model is equivalent to the model of two independent random samples of which we are interested in the difference of their means. This is the unpaired Student’s t test.\\

\textbf{Factor Variables}\\
If a discrete variable has more than two levels it is called a factor.\\
To be able to use such a factor in a regression model we have to introduce for every level a so-called dummy-variable
\begin{equation}
  x_k^{(j)} = \left(x_{k,1}^{(j)}, \dots ,x_{k,i}^{(j)}, \dots, x_{k,n}^{(j)}\right).
\end{equation}
Now we can add these $s - 1$ dummy variables to the multiple linear regression model
\begin{equation}
  y = \beta_0 + \beta_1 x_1 + \dots + (\beta_{k,2}x^{(2)}_k + \dots + \beta_{k,s}x^{(s)}_k) + \dots + \beta_m x_m + \varepsilon.
\end{equation}
Realise that for a factor with $s$ levels we only added $s - 1$ dummy-variables. This is done to make the estimation unique by setting artificially $\beta_{k,1} = 0$.
Example:
\begin{align*}
  \scriptstyle
  &\texttt{Position}:      & &\texttt{int}  &\texttt{1 1 1 2 2 1 \dots\; 3 3 4 \dots\; 4 3 \dots\; 3}\\
  &\dots\\
  &\texttt{Position1.dum}: & &\texttt{num}  &\texttt{1 1 1 0 0 1 \dots\; 0 0 0 \dots\; 0 0 \dots\; 0}\\
  &\texttt{Position2.dum}: & &\texttt{num}  &\texttt{0 0 0 1 1 0 \dots\; 0 0 0 \dots\; 0 0 \dots\; 0}\\
  &\texttt{Position3.dum}: & &\texttt{num}  &\texttt{0 0 0 0 0 0 \dots\; 1 1 0 \dots\; 0 1 \dots\; 1}\\
  &\texttt{Position4.dum}: & &\texttt{num}  &\texttt{0 0 0 0 0 0 \dots\; 0 0 1 \dots\; 1 0 \dots\; 0}
\end{align*}

\subsection{Comparison of Regression Models with $\mathbf{F}$-Test}
\noindent\rule[\linienAbstand]{\linewidth}{\linienDicke}
We need a test which allows us to ask the question if a certain factor has a significant influence. We cannot simply look at the $t$-statistics and P-values of individual levels in the R-output. We need to be able to test not only one coefficient but a whole set of coefficients simultaneously.\\
In general we want to compare two multiple linear regression models. To do that we fit a big
model with $p = m + 1$ coefficients and a smaller model with $q$ coefficients $\beta_{j1} , . . . , \beta_{jq}$ put to zero, i.e. with only $p - q$ coefficients.\\

The question is, if the smaller regression model can fit the data as well as the big one? The alternative hypothesis to that question are
\begin{equation}
  \begin{split}
    H_0:& \beta_{j1} = \beta_{j2} = \dots \beta_{j_q} = 0\\
    H_1:& \beta_{j_k} \neq \text{for at least one} k \in \{1,\dots,q\}
  \end{split}
\end{equation}

The $F$-test to compare two models tests whether any of the explanatory variables in a multiple linear regression model are significant.\\
To answer this question we calculate the $F$-statistic
\begin{equation}
  F = \frac{n-p}{q}\frac{\textup{SS}^*_E - \textup{SS}_E}{\textup{SS}_E}
\end{equation}
where $\textup{SS}^*_E$ is the sum of squares of the errors of the small model and $\textup{SS}_E$ is the sum of squares of the errors of the big model. Under the null hypothesis the $F$-statistic follows an $F$-distribution with $(q, n - p)$ degrees of freedom.
